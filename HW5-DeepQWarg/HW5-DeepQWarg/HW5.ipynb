{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5: Petting a deep Q warg\n",
    "\n",
    "This homework builds on the same game as homework 4. \n",
    "\n",
    "![](figures/PetAWarg.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to solve this homework\n",
    "The following problems you can solve either with the help of an LLM or by hand. \n",
    "\n",
    "* If you are solving by hand, make sure that you add sufficient comments to make sure that the code is understandable. \n",
    "* If you are solving using an LLM, add in form of comments\n",
    "    * the LLM used (at the first use instance)\n",
    "    * the prompt used to elicit the code\n",
    "    * modifications that had to be done to the code \n",
    "\n",
    "For example:\n",
    "\n",
    "```\n",
    "# --- LLM used: ChatGPT 4.5\n",
    "# --- LLM prompt\n",
    "# Write a python class to encapsulate the least common multiple algorithm\n",
    "# --- End of LLM prompt\n",
    "```\n",
    "\n",
    "The programming language should be Python.\n",
    "\n",
    "You can reuse code from your submission for homework 4. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P1: Model the game as an environment in gymnasium\n",
    "\n",
    "gymnasium (https://gymnasium.farama.org/index.html) is a fork of the OpenAI gym library. It is a library that allows you to easily build environments \n",
    "\n",
    "Model the PetAWarg game as an environment in gymnasium. You don't have to create visual framework: it is enought to implement the render function to print the current state. \n",
    "\n",
    "NOTE: If you are using a LLM, you should be able to ask it to convert your previous implementation into the implementation in gym. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting observation: [-0.00554305 -0.00649236 -0.03758243 -0.03463649]\n",
      "Episode finished! Total reward: 17.0\n"
     ]
    }
   ],
   "source": [
    "# Run `pip install \"gymnasium[classic-control]\"` for this example.\n",
    "import gymnasium as gym\n",
    "\n",
    "# Create our training environment - a cart with a pole that needs balancing\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "\n",
    "# Reset environment to start a new episode\n",
    "observation, info = env.reset()\n",
    "# observation: what the agent can \"see\" - cart position, velocity, pole angle, etc.\n",
    "# info: extra debugging information (usually not needed for basic learning)\n",
    "\n",
    "print(f\"Starting observation: {observation}\")\n",
    "# Example output: [ 0.01234567 -0.00987654  0.02345678  0.01456789]\n",
    "# [cart_position, cart_velocity, pole_angle, pole_angular_velocity]\n",
    "\n",
    "episode_over = False\n",
    "total_reward = 0\n",
    "\n",
    "while not episode_over:\n",
    "    # Choose an action: 0 = push cart left, 1 = push cart right\n",
    "    action = env.action_space.sample()  # Random  for now - real agents will be smarter!\n",
    "\n",
    "    # Take the action and see what happens\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # reward: +1 for each step the pole stays upright\n",
    "    # terminated: True if pole falls too far (agent failed)\n",
    "    # truncated: True if we hit the time limit (500 steps)\n",
    "\n",
    "    total_reward += reward\n",
    "    episode_over = terminated or truncated\n",
    "\n",
    "print(f\"Episode finished! Total reward: {total_reward}\")\n",
    "\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P2: Pet, strike, pet, strike, pet\n",
    "\n",
    "Using the environment class implemented above, create an instance of the environment. Print out its state (by calling render()). \n",
    "\n",
    "Then, perform the actions: pet, strike, pet, strike, pet. After each action, print out the state.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P3: DQN\n",
    "\n",
    "Install the stable_baselines3 library. Using the DQN implementation from that library, train an MlpPolicy policy for playing the PetAWarg game. \n",
    "\n",
    "https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P4: Print out the policy learned by DQN\n",
    "\n",
    "Print out the policy learned by DQN in the previous step. You can assume that the policy is deterministic. In this case, the policy can be printed out by iterating over all the states and printing out the action generated by the policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
