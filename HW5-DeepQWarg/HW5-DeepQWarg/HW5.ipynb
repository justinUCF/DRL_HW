{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5: Petting a deep Q warg\n",
    "\n",
    "This homework builds on the same game as homework 4. \n",
    "\n",
    "![](figures/PetAWarg.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to solve this homework\n",
    "The following problems you can solve either with the help of an LLM or by hand. \n",
    "\n",
    "* If you are solving by hand, make sure that you add sufficient comments to make sure that the code is understandable. \n",
    "* If you are solving using an LLM, add in form of comments\n",
    "    * the LLM used (at the first use instance)\n",
    "    * the prompt used to elicit the code\n",
    "    * modifications that had to be done to the code \n",
    "\n",
    "For example:\n",
    "\n",
    "```\n",
    "# --- LLM used: ChatGPT 4.5\n",
    "# --- LLM prompt\n",
    "# Write a python class to encapsulate the least common multiple algorithm\n",
    "# --- End of LLM prompt\n",
    "```\n",
    "\n",
    "The programming language should be Python.\n",
    "\n",
    "You can reuse code from your submission for homework 4. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P1: Model the game as an environment in gymnasium\n",
    "\n",
    "gymnasium (https://gymnasium.farama.org/index.html) is a fork of the OpenAI gym library. It is a library that allows you to easily build environments \n",
    "\n",
    "Model the PetAWarg game as an environment in gymnasium. You don't have to create visual framework: it is enought to implement the render function to print the current state. \n",
    "\n",
    "NOTE: If you are using a LLM, you should be able to ask it to convert your previous implementation into the implementation in gym. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "class PetAWargEnv(gym.Env):\n",
    "\n",
    "    # States\n",
    "    SLEEPING = 0\n",
    "    ANGRY = 1\n",
    "    FURIOUS = 2\n",
    "    APOPLECTIC = 3\n",
    "    SAFE = 4\n",
    "    SORRY = 5\n",
    "\n",
    "    # Actions\n",
    "    PET = 0\n",
    "    STRIKE = 1\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.observation_space = gym.spaces.Discrete(6)\n",
    "        self.action_space = gym.spaces.Discrete(2)\n",
    "\n",
    "        self.render_mode = None\n",
    "\n",
    "        self.state_names = {self.SLEEPING: \"Sleeping\", self.ANGRY: \"Angry\", self.FURIOUS: \"Furious\", self.APOPLECTIC: \"Apoplectic\", self.SAFE: \"Safe\", self.SORRY: \"Sorry\"}\n",
    "        self.action_names = {self.PET: \"Pet\", self.STRIKE: \"Strike\"}\n",
    "\n",
    "        self.state = None\n",
    "        self.score = None\n",
    "        self.steps = 0\n",
    "\n",
    "    def reset(self, seed = None, options = None):\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        self.state = self.SLEEPING\n",
    "        self.score = 0\n",
    "        self.steps = 0\n",
    "\n",
    "        return self.state, self.score\n",
    "    \n",
    "    def step(self, action):\n",
    "        if self.state is None:\n",
    "            raise RuntimeError(\"Environment not initialized. Call reset() first.\")\n",
    "        \n",
    "        self.steps += 1\n",
    "        reward = 0\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        \n",
    "        # State transitions based on current state and action\n",
    "        if self.state == self.SLEEPING:\n",
    "            if action == self.PET:\n",
    "                # Pet with p=0.05 -> Safe, with p=0.95 -> Angry\n",
    "                if self.np_random.random() < 0.05:\n",
    "                    self.state = self.SAFE\n",
    "                    self.score = 10\n",
    "                else:\n",
    "                    self.state = self.ANGRY\n",
    "            elif action == self.STRIKE:\n",
    "                # Strike with p=1.0 -> Angry\n",
    "                self.state = self.ANGRY\n",
    "                \n",
    "        elif self.state == self.ANGRY:\n",
    "            if action == self.PET:\n",
    "                # Pet with p=1.0 -> Sorry\n",
    "                self.state = self.SORRY\n",
    "                self.score = -10\n",
    "            elif action == self.STRIKE:\n",
    "                # Strike with p=1.0 -> Furious\n",
    "                self.state = self.FURIOUS\n",
    "                \n",
    "        elif self.state == self.FURIOUS:\n",
    "            if action == self.PET:\n",
    "                # Pet with p=1.0 -> Sorry\n",
    "                self.state = self.SORRY\n",
    "                self.score = -10\n",
    "            elif action == self.STRIKE:\n",
    "                # Strike with p=1.0 -> Apoplectic\n",
    "                self.state = self.APOPLECTIC\n",
    "                \n",
    "        elif self.state == self.APOPLECTIC:\n",
    "            if action == self.PET:\n",
    "                # Pet with p=1.0 -> Sorry\n",
    "                self.state = self.SORRY\n",
    "                self.score = -10\n",
    "            elif action == self.STRIKE:\n",
    "                # Strike with p=0.2 -> Safe, with p=0.8 -> Sorry\n",
    "                if self.np_random.random() < 0.2:\n",
    "                    self.state = self.SAFE\n",
    "                    self.score = 10\n",
    "                else:\n",
    "                    self.state = self.SORRY\n",
    "                    self.score = -10\n",
    "                    \n",
    "        elif self.state == self.SAFE:\n",
    "            # Terminal state - should not be here\n",
    "            terminated = True\n",
    "            \n",
    "        elif self.state == self.SORRY:\n",
    "            # Terminal state - should not be here\n",
    "            terminated = True\n",
    "        \n",
    "        # Set reward based on score\n",
    "        reward = self.score\n",
    "        \n",
    "        # Episode terminates if we reach Safe or Sorry states\n",
    "        if self.state == self.SAFE or self.state == self.SORRY:\n",
    "            terminated = True\n",
    "        \n",
    "    \n",
    "        self.render()\n",
    "        \n",
    "        info = {\n",
    "            \"score\": self.score,\n",
    "            \"steps\": self.steps,\n",
    "            \"state_name\": self.state_names[self.state]\n",
    "        }\n",
    "        \n",
    "        return self.state, reward, terminated, truncated, info\n",
    "    \n",
    "    def render(self):\n",
    "        state_name = self.state_names[self.state]\n",
    "        print(f\"\\nStep: {self.steps}\")\n",
    "        print(f\"Current State: {state_name}\")\n",
    "        print(f\"Current Score\\n: {self.score:+d}\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P2: Pet, strike, pet, strike, pet\n",
    "\n",
    "Using the environment class implemented above, create an instance of the environment. Print out its state (by calling render()). \n",
    "\n",
    "Then, perform the actions: pet, strike, pet, strike, pet. After each action, print out the state.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting PetAWarg Game!\n",
      "Actions: 0=Pet, 1=Strike\n",
      "\n",
      "Current State: Sleeping\n",
      "Action taken: Pet\n",
      "\n",
      "Step: 1\n",
      "Current State: Angry\n",
      "Current Score\n",
      ": +0\n",
      "\n",
      "Current State: Angry\n",
      "Action taken: Strike\n",
      "\n",
      "Step: 2\n",
      "Current State: Furious\n",
      "Current Score\n",
      ": +0\n"
     ]
    }
   ],
   "source": [
    "# Create environment\n",
    "env = PetAWargEnv()\n",
    "\n",
    "# Reset environment\n",
    "state, info = env.reset(seed=42)\n",
    "\n",
    "print(\"\\nStarting PetAWarg Game!\")\n",
    "print(\"Actions: 0=Pet, 1=Strike\")\n",
    "\n",
    "# Run a sample episode with random actions\n",
    "terminated = False\n",
    "truncated = False\n",
    "total_reward = 0\n",
    "\n",
    "actions = {0,1,0,1,0}\n",
    "\n",
    "print(f\"Initial State: {env.state_names[env.state]}\")\n",
    "for action in actions:\n",
    "    action_name = env.action_names[action]\n",
    "    print(f\"\\nCurrent State: {env.state_names[env.state]}\")\n",
    "    print(f\"Action taken: {action_name}\")\n",
    "    state, reward, terminated, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    if terminated:\n",
    "        print(\"Episode terminated!\")\n",
    "        print(f\"Final State: {info['state_name']}\")\n",
    "        print(f\"Total Steps: {info['steps']}\")\n",
    "        print(f\"Total Reward: {total_reward:+d}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P3: DQN\n",
    "\n",
    "Install the stable_baselines3 library. Using the DQN implementation from that library, train an MlpPolicy policy for playing the PetAWarg game. \n",
    "\n",
    "https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P4: Print out the policy learned by DQN\n",
    "\n",
    "Print out the policy learned by DQN in the previous step. You can assume that the policy is deterministic. In this case, the policy can be printed out by iterating over all the states and printing out the action generated by the policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
